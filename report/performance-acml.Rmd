---
title: "Performace of ACML"
author: "Shawn Garbett"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

<style type="text/css">
.failure {background-color: pink;}

body, td {
   font-size: 12px;
}
code.r{
  font-size: 12px;
}
pre {
  font-size: 10px
}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tangram)
library(Hmisc)
library(purrr)
library(boot)
```

```{r functions, include=FALSE}
source("../setup.R")
scenarios <- inits.sim

# Helper function for repeated operation over files.
load.results <- function(target="../output", verbose=FALSE)
{
  files <- dir(target)
  files <- paste0(target, "/", files[grepl("^run", files)])
  
  do.call(rbind, lapply(files, function(f) {
    if(verbose) cat("Loading ", f, "\n")
    load(f)
    
    cat(f, results$Scenario)
    results
  }))
}
results <- load.results(verbose=TRUE)

results <- cleanup.import(
  results,
  labels= c("Job", "Scenario", "Sampling", "Method", 
            rep(c("Intercept", "Time", "Group", "Confounder", "Interaction"), times=4)
            )
)

results$Sampling <- factor(results$Sampling,
                           levels=c("Random", "Slope", "Mix1", "Mix2", "Intercept"))

```

## Purpose

In preparing for a CRAN packaging and improvements of the ODS4LDA (Outcome Dependent Sampling for Longitudinal Data Analysis) code it is desirable to understand the statistical performance of the proposed estimators. As such, this document examines the bias, precision, coverage and accuracy of the library for a variety of cases.

### Reproducibility

The code being tested is available in the git commit hash: `r system("git log --pretty=format:'%H' -n 1", intern=TRUE)` at the following location `github.com/spgarbet/ods4lda-sim.git`.

### Method

`r N` individuals were simulated using `r n` mixed effect models,

```{r, results='asis'}
cat("hn_scale for two phase was set to ")
cat(paste0(hn_scale, collapse=", "), ".\n", sep='')
```

```{r, results='asis'}
for(i in 1:n)
{
  cat("### Model Run",i,"\n\n")
  parm <- inits.sim[i,]
  cat("$$y_{ij} = ",parm[1], parm[3]," g_i ", parm[5]," g_i t_{ij}", parm[2]," t_{ij} ", parm[4]," c_i + b_{0i} + b_{1i} t_{ij} + \\epsilon_{ij}$$\n", sep='')
  cat("$$\\begin{bmatrix} b_{0i} \\\\ b_{1i} \\\\ \\end{bmatrix} \\sim \\mathcal{N}\\left(\\begin{bmatrix} 0\\\\0\\\\ \\end{bmatrix},\\begin{bmatrix} ", exp(parm[6])^2,"& 0 \\\\ 0 & ", round(exp(parm[7])^2,4), " \\\\ \\end{bmatrix} \\right)$$\n")
  cat("$$ \\epsilon_{ij} \\sim \\mathcal{N}(0, ", round(exp(parm[9])^2,4), ")$$")
  cat("\n\n")
}

```



where $t_{ij}$ represents the $j$th timepoint for the $i$th individual, $g_i$ is the presence or absence of a SNIP or expensive binomial information for the $i$th individual, $c_i$ is the confounder for the $i$th individual.

Membership in the $g_i$ group occurs at an average `r paste0(prev.grp, collapse=", ")` probability.
In addition $c_i \sim \mathcal{N}(0,1)$ and membership probabilities were computed with $\text{logit}^{-1}(\alpha + \beta c_i^2)$ with $\alpha$ adjusted to make the average. 

```{r, results='asis'}
cat("$$\\alpha = {", paste(round(conf.param.sim[,1],2), collapse=", ")  ,"}$$\n\n")
cat("$$\\beta = {", paste(round(conf.param.sim[,2],2), collapse=", ")  ,"}$$\n\n")
```

This was used to create between 4 to 6 observations at equally spaced time points for each individual.

During each of the `r N` runs approximately `r paste0(rowSums(NsPerStratumUniv.sim), collapse=", ")` individuals were sampled using a Bernoulli strategy for stratum. `r paste0("(", paste0(sapply(1:n, function(i) paste0(NsPerStratumUniv.sim[i,], collapse=", ")), collapse="), ("), ")")` specifically. Cutoffs quantiles for sampling were as follows: `r paste0("[",paste(100*(1-p.central)/2, 100-100*(1-p.central)/2, sep=","),"]", collapse=", ")`

From these runs, we examine the 95\% coverage intervals, the observed bias of estimation and compare the variance of the estimates to the average of the variances estimated by the model using both methods on the same subsampled data sets, as well as examine the accuracy and precision.

The fitting is done by a variety of different methods. Each of the combinations of 4 sampling methods: random, intercept, slope and mixture1 and mixture2 crossed with fitting methods: ACML, weighted likelihood, direct multiple imputation (`r n.imp` replicates) and Two Phase exluding one duplicate method for random sampling (wl).

## Results

### Bias


```{r, results='asis', echo=FALSE}

cell_func <- function(table, row, column, fun=NULL, ...)
{
  colname <- NULL
  try({colname <- derive_label(column)}, silent=TRUE)
  if(is.null(colname)) try({colname <- derive_label(column$left)}, silent=TRUE)
  if(is.null(colname)) colname <- column$string()
  table <- col_header(table, colname)
  major <- unique(row$left$data)
  minor <- levels(row$right$data)

  for(m in major)
  {
    first <- m
    for(n in minor)
    {
      table <- row_header(table, list(first, n), sub=FALSE)
      first <- ""
      table <- add_row(table, fun(column, m == row$left$data & n == row$right$data))
    }
  }
  
  table
}

est.bias      <- function(column, selector)
{
  data  <- column$data[selector]
  do.test <- length(data) > 2
  cls <- NULL
  ci <- if(do.test)
  {
    test  <- t.test(data, na.rm=TRUE)
    cls   <- if(test$p.value < 0.001) "failure"
    paste0(render_f(test$estimate, 3), 
           " (", render_f(test$conf.int[1], 3), ", ",
           render_f(test$conf.int[2], 3), ")")
  }
  else ""
  
  cell(ci, class=cls)
}

bias_table <- function(m)
{
  html5(tangram(diff.int + diff.time + diff.grp + diff.conf + diff.tg ~ Scenario*Sampling,
         data=subset(results, Method==m), transforms = cell_func, fun=est.bias, id=paste0(m, "_bias"),
        caption=paste(m, "Estimator Bias"), style="nejm",
         footnote="Values in Pink have p < 0.001"))
}

all_bias_table <- function()
{
   all <- paste0(sapply(c("ACML", "WL", "MI", "TwoPhase"), function(m) bias_table(m)), collapse='<br/>')
   class(all) <- c("html", "character")
   all
}

all_bias_table()

```
<br/>

```{r, , results='asis', echo=FALSE}
var.bias <- function(column, selector)
{
  truth    <- var(column$left$data[selector])   # variance of estimates
  estimate <- mean(column$right$data[selector]) # mean of estimated variances

  percent  <- 100*(estimate-truth)/truth
  cls      <- if(is.na(percent) || abs(percent) > 10) "failure" else NULL
  f_per    <- if(is.na(percent)) "-" else render_f(percent, 1)
  cell(f_per, class=cls)
}

var.bias_table <- function(m)
{
  html5(tangram(est.int * var.int    +
                   est.time* var.time + 
                   est.grp * var.grp  +
                   est.conf* var.conf +
                   est.tg  * var.tg   ~ Scenario*Sampling,
           data=subset(results, Method==m),
           transforms = cell_func, fun=var.bias,
           id=paste0(m, "_varbias"),
           caption=paste(m, "Variance Bias Relative Percent"),
           style="nejm",
           footnote="Values in Pink have exceed 10%"))
}

all_var.bias_table <- function()
{
   all <- paste0(sapply(c("ACML", "WL", "MI", "TwoPhase"), function(m) var.bias_table(m)), collapse='<br/>')
   class(all) <- c("html", "character")
   all
}

all_var.bias_table()
```

### Coverage

```{r, results='asis', echo=FALSE}
coverage <- function(table,
                     row,
                     column,
                     ...)
{
  grid          <- table(as.categorical(row$data), as.categorical(column$data), useNA="no")
  validcol      <- which(!apply(grid,2,FUN = function(x){all(x == 0)}))
  validrow      <- which(!apply(grid,1,FUN = function(x){all(x == 0)}))
  ncol          <- dim(grid)[2]
  nrow          <- dim(grid)[1]
  

  denominators  <- matrix(rep(colSums(grid), nrow), ncol=ncol, byrow=TRUE)
  rowlabels     <- rownames(grid)

  # Compute overall N values for each category
  # length(datac[datac == cat & !is.na(datac)])
  subN <- lapply(colnames(grid), FUN=function(cat)
    cell_n( sum(column$data == cat, na.rm=TRUE), subcol=cat)
  )

  # Why is this so difficult?

  # More complex name derivation
  name <- row$name()
  try({
        l2 <- attr(row$data, "label")
        if(!is.null(l2)) {name<-l2}
  })

  # Select part of grid table, then do all the munging to get it back in form
  x <- matrix(grid[2,], nrow=1)
  colnames(x) <- colnames(grid)
  rownames(x) <- name
  grid <- x
  denominators <- matrix(denominators[2,], nrow=1)
  nrow <- 1

  # Column Headers
  table <- col_header(table, colnames(grid))
  table <- col_header(table, subN)

    # Row Headers
  if(nrow > 1) table <- row_header(table, derive_label(row)) # Deal with single
  for(nm in rownames(grid)) table <- row_header(table, nm)

  # Now loop the grid into the table as a fraction
  for(j in 1:ncol)
  {
    if(nrow > 1) table <- add_row(table, "")
    for(i in 1:nrow)
    {
      table <-
        if(denominators[i,j] == 0)
          add_row(table, "")
        else
        {
          test <- suppressWarnings(prop.test(grid[i,j], denominators[i,j], p=0.95))
          
          cls <- if(test$p.value < 0.001) "failure" else NULL
          add_row(table, cell(paste0(
            render_f(test$estimate*100, 1), " (",
            render_f(test$conf.int[1]*100, 1), ", ",
            render_f(test$conf.int[2]*100, 1), ")"
          ), class=cls))
        }
    }
    table <- new_col(table)
  }

  table
}

cover_table <- function(sample, subbatch)
{
  html5(tangram(
           Method ~ cover.int+cover.time+cover.grp+cover.conf+cover.tg,
           subset(results, Sampling==sample & Scenario == subbatch),
           transforms=coverage,
           id=paste0(sample,"_",subbatch),
           caption=paste(sample, "Sampling Coverage Scenario", subbatch),
           style="nejm", footnote="Values in Pink have p < 0.001"))
}

all_cover_table <- function()
{
   all <- paste0(sapply(1:n,
                        function(x) sapply(levels(results$Sampling),
                                           function(m) paste0(cover_table(m, x), collapse='<br/>'))
                       ),
                 collapse='<br/>')
   class(all) <- c("html", "character")
   all
}

all_cover_table()
```

### Efficiency

```{r efficiency, echo=FALSE}

ref <- subset(results, Sampling=="Random" & Method=="ACML")

efficiency <- function(table, row, column, ref, scenario, ...)
{
  ran        <- mean(ref[,column$right$string()]) # Comparison
  
  datar      <- as.categorical(row$data)
  categories <- levels(datar)

  table <- col_header(table, derive_label(column$right))
  table <- row_header(table, scenario)
  table <- add_row(table, "")
  for(i in categories)
  {
    table <- row_header(table, paste0("  ", i))

    result <- paste0(
      sapply(unique(column$left$data), function(method) {
       selector <- datar==i & (column$left$data == method)
       render_f(ran/mean(column$right$data[selector]),2)
       }),
      collapse=", ")

    table <- add_row(table, paste0("(",result,")") )
  }
  
  table
}

eff.inner <- function(m, sc)
{
  tangram(Method*(var.int+var.time+var.grp+var.conf+var.tg) ~ Sampling,
          data=subset(results, Method %in% m  & Scenario==sc),
          ref =subset(ref,     Method=="ACML" & Scenario==sc),
          scenario=sc,
          transforms=efficiency, id="tmp")
}
eff <- function(m)
{
  tbl <- purrr::reduce(lapply(1:length(prev.grp), function(i) eff.inner(m, i)), rbind)
    
  attr(tbl, "caption") <- paste0("Efficiency (", paste0(m, collapse=", "), ")")
  attr(tbl, "style")   <- "nejm"

  tbl
}

```

```{r eff_acml, results='asis', echo=FALSE}
html5(eff(c("ACML", "WL", "MI", "TwoPhase")))
```



Thus concludes this round of simulation.

